/*
 * **************************************************-
 * ingrid-mdek-job
 * ==================================================
 * Copyright (C) 2014 - 2024 wemove digital solutions GmbH
 * ==================================================
 * Licensed under the EUPL, Version 1.2 or â€“ as soon they will be
 * approved by the European Commission - subsequent versions of the
 * EUPL (the "Licence");
 * 
 * You may not use this work except in compliance with the Licence.
 * You may obtain a copy of the Licence at:
 * 
 * https://joinup.ec.europa.eu/software/page/eupl
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the Licence is distributed on an "AS IS" basis,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the Licence for the specific language governing permissions and
 * limitations under the Licence.
 * **************************************************#
 */
package de.ingrid.mdek.job;

import de.ingrid.elasticsearch.ElasticConfig;
import de.ingrid.elasticsearch.IBusIndexManager;
import de.ingrid.elasticsearch.IndexManager;
import de.ingrid.iplug.dsc.index.DscDocumentProducer;
import de.ingrid.iplug.dsc.record.DscRecordCreator;
import de.ingrid.mdek.MdekError;
import de.ingrid.mdek.MdekError.MdekErrorType;
import de.ingrid.mdek.MdekKeys;
import de.ingrid.mdek.MdekUtils;
import de.ingrid.mdek.MdekUtils.*;
import de.ingrid.mdek.caller.IMdekCaller.FetchQuantity;
import de.ingrid.mdek.job.tools.MdekIdcEntityComparer;
import de.ingrid.mdek.services.catalog.MdekCatalogService;
import de.ingrid.mdek.services.catalog.MdekObjectService;
import de.ingrid.mdek.services.log.ILogService;
import de.ingrid.mdek.services.persistence.db.DaoFactory;
import de.ingrid.mdek.services.persistence.db.dao.IObjectNodeDao;
import de.ingrid.mdek.services.persistence.db.dao.IT01ObjectDao;
import de.ingrid.mdek.services.persistence.db.mapper.BeanToDocMapperSecurity;
import de.ingrid.mdek.services.persistence.db.mapper.IMapper.MappingQuantity;
import de.ingrid.mdek.services.persistence.db.model.*;
import de.ingrid.mdek.services.security.IPermissionService;
import de.ingrid.mdek.services.utils.*;
import de.ingrid.mdek.services.utils.MdekPermissionHandler.GroupType;
import de.ingrid.utils.ElasticDocument;
import de.ingrid.utils.IngridDocument;
import de.ingrid.utils.IngridHit;
import de.ingrid.utils.dsc.Record;
import de.ingrid.utils.tool.XsltUtils;
import de.ingrid.utils.xml.XMLUtils;
import org.apache.commons.lang.exception.ExceptionUtils;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.stereotype.Service;
import org.w3c.dom.Document;
import org.w3c.dom.Node;

import java.util.ArrayList;
import java.util.Date;
import java.util.List;
import java.util.Stack;

/**
 * Encapsulates all Job functionality concerning OBJECTS.
 */
@Service
public class MdekIdcObjectJob extends MdekIdcJob {

    private MdekCatalogService catalogService;
    private MdekObjectService objectService;

    private MdekPermissionHandler permissionHandler;
    private MdekWorkflowHandler workflowHandler;
    private MdekTreePathHandler pathHandler;

    private IObjectNodeDao daoObjectNode;
    private IT01ObjectDao daoT01Object;

    protected BeanToDocMapperSecurity beanToDocMapperSecurity;

    @Autowired
    @Qualifier("dscRecordCreator")
    private DscRecordCreator dscRecordProducer;

    private XsltUtils xsltUtils;

    @Autowired
    public MdekIdcObjectJob(ILogService logService,
                            DaoFactory daoFactory,
                            IPermissionService permissionService,
                            ElasticConfig elasticConfig,
                            IndexManager indexManager,
                            IBusIndexManager iBusIndexManager) {
        super(logService.getLogger(MdekIdcObjectJob.class), daoFactory);

        catalogService = MdekCatalogService.getInstance(daoFactory);
        objectService = MdekObjectService.getInstance(daoFactory, permissionService);
//		addressService = MdekAddressService.getInstance(daoFactory, permissionService);

        permissionHandler = MdekPermissionHandler.getInstance(permissionService, daoFactory);
        workflowHandler = MdekWorkflowHandler.getInstance(permissionService, daoFactory);
        pathHandler = MdekTreePathHandler.getInstance(daoFactory);

        daoObjectNode = daoFactory.getObjectNodeDao();
        daoT01Object = daoFactory.getT01ObjectDao();

        this.indexManager = elasticConfig.esCommunicationThroughIBus ? iBusIndexManager : indexManager;

        beanToDocMapperSecurity = BeanToDocMapperSecurity.getInstance(daoFactory, permissionService);

        xsltUtils = new XsltUtils();
    }

    public IngridDocument getTopObjects(IngridDocument params) {
        try {
            daoObjectNode.beginTransaction();
            daoObjectNode.disableAutoFlush();

            String userUuid = getCurrentUserUuid(params);

            // fetch top Objects
            List<ObjectNode> oNs = daoObjectNode.getTopObjects(IdcEntityVersion.WORKING_VERSION, true);

            ArrayList<IngridDocument> resultList = new ArrayList<IngridDocument>(oNs.size());
            for (ObjectNode oN : oNs) {
                IngridDocument objDoc = new IngridDocument();
                beanToDocMapper.mapObjectNode(oN, objDoc, MappingQuantity.TREE_ENTITY);
                beanToDocMapper.mapT01Object(oN.getT01ObjectWork(), objDoc, MappingQuantity.TREE_ENTITY);

                // add permissions the user has on given object !
                List<Permission> perms = permissionHandler.getPermissionsForObject(oN.getObjUuid(), userUuid, true);
                beanToDocMapperSecurity.mapPermissionList(perms, objDoc);

                resultList.add(objDoc);
            }

            daoObjectNode.commitTransaction();

            IngridDocument result = new IngridDocument();
            result.put(MdekKeys.OBJ_ENTITIES, resultList);

            return result;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            throw handledExc;
        }
    }

    public IngridDocument getSubObjects(IngridDocument params) {
        try {
            String userUuid = getCurrentUserUuid(params);
            String uuid = (String) params.get(MdekKeys.UUID);

            daoObjectNode.beginTransaction();
            daoObjectNode.disableAutoFlush();

            List<ObjectNode> oNs =
                    daoObjectNode.getSubObjects(uuid, IdcEntityVersion.WORKING_VERSION, true);

            ArrayList<IngridDocument> subObjDocs = new ArrayList<IngridDocument>(oNs.size());
            for (ObjectNode oN : oNs) {
                IngridDocument objDoc = new IngridDocument();
                beanToDocMapper.mapObjectNode(oN, objDoc, MappingQuantity.TREE_ENTITY);
                beanToDocMapper.mapT01Object(oN.getT01ObjectWork(), objDoc, MappingQuantity.TREE_ENTITY);

                // add permissions the user has on given object !
                List<Permission> perms =
                        permissionHandler.getPermissionsForObject(oN.getObjUuid(), userUuid, true);
                beanToDocMapperSecurity.mapPermissionList(perms, objDoc);

                subObjDocs.add(objDoc);
            }

            daoObjectNode.commitTransaction();

            IngridDocument result = new IngridDocument();
            result.put(MdekKeys.OBJ_ENTITIES, subObjDocs);
            return result;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            throw handledExc;
        }
    }

    public IngridDocument getObjectPath(IngridDocument params) {
        try {
            daoObjectNode.beginTransaction();
            daoObjectNode.disableAutoFlush();

            String uuid = (String) params.get(MdekKeys.UUID);
            List<String> uuidList = daoObjectNode.getObjectPath(uuid);

            IngridDocument result = new IngridDocument();
            result.put(MdekKeys.PATH, uuidList);

            daoObjectNode.commitTransaction();
            return result;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            throw handledExc;
        }
    }

    public IngridDocument getObjDetails(IngridDocument params) {
        try {
            String userUuid = getCurrentUserUuid(params);
            String uuid = (String) params.get(MdekKeys.UUID);
            IdcEntityVersion whichEntityVersion =
                    (IdcEntityVersion) params.get(MdekKeys.REQUESTINFO_WHICH_ENTITY_VERSION);
            FetchQuantity fetchQuantity =
                    (FetchQuantity) params.get(MdekKeys.REQUESTINFO_FETCH_QUANTITY);

            daoObjectNode.beginTransaction();
            daoObjectNode.disableAutoFlush();

            if (log.isDebugEnabled()) {
                log.debug("Invoke getObjDetails (uuid='" + uuid + "').");
            }
            IngridDocument result = objectService.getObjectDetails(uuid,
                    whichEntityVersion, fetchQuantity, userUuid);

            daoObjectNode.commitTransaction();

            return result;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            throw handledExc;
        }
    }

    /**
     * Fetches WORKING VERSION !
     */
    private IngridDocument getObjDetails(String objUuid, String userUuid) {
        return objectService.getObjectDetails(objUuid,
                IdcEntityVersion.WORKING_VERSION, FetchQuantity.EDITOR_ENTITY, userUuid);
    }

    public IngridDocument getInitialObject(IngridDocument oDocIn) {
        String userUuid = getCurrentUserUuid(oDocIn);
        try {
            daoObjectNode.beginTransaction();
            daoObjectNode.disableAutoFlush();

            // take over data from parent (if set)
            String parentUuid = oDocIn.getString(MdekKeys.PARENT_UUID);
            if (parentUuid != null) {
                ObjectNode pNode = objectService.loadByUuid(parentUuid, IdcEntityVersion.WORKING_VERSION);
                if (pNode == null) {
                    throw new MdekException(new MdekError(MdekErrorType.UUID_NOT_FOUND));
                }

                T01Object oParent = pNode.getT01ObjectWork();

                // supply separate parent info
                beanToDocMapper.mapObjectParentData(oParent, oDocIn);

                // take over initial data from parent
                beanToDocMapper.mapT01Object(oParent, oDocIn, MappingQuantity.INITIAL_ENTITY);
            }
			
/*
			// Address set ? Set calling user as "Ansprechpartner" if nothing set, see INGRID32-46
			// NO !!! calling user is now hidden address ! cannot be set as reference, see INGRID32-36
			if (!objectService.hasAddressReference(oDocIn)) {
				AddressNode addrNode = addressService.loadByUuid(userUuid, IdcEntityVersion.WORKING_VERSION);
				addPointOfContactAddress(oDocIn, addrNode);
			}
*/
            // take over spatial reference from catalog
            T03Catalogue catalog = catalogService.getCatalog();
            IngridDocument catalogDoc = new IngridDocument();
            beanToDocMapper.mapT03Catalog(catalog, catalogDoc);
            IngridDocument catDocLoc = (IngridDocument) catalogDoc.get(MdekKeys.CATALOG_LOCATION);
            if (catDocLoc != null) {
                ArrayList<IngridDocument> locList = new ArrayList<IngridDocument>(1);
                locList.add(catDocLoc);
                oDocIn.put(MdekKeys.LOCATIONS, locList);
            }

            // add permissions the user has on initial object !
            List<Permission> perms = permissionHandler.getPermissionsForInitialObject(oDocIn, userUuid);
            beanToDocMapperSecurity.mapPermissionList(perms, oDocIn);

            daoObjectNode.commitTransaction();
            return oDocIn;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            throw handledExc;
        }
    }

    public IngridDocument getWorkObjects(IngridDocument params) {
        String userUuid = getCurrentUserUuid(params);
        try {
            IdcWorkEntitiesSelectionType selectionType =
                    (IdcWorkEntitiesSelectionType) params.get(MdekKeys.REQUESTINFO_ENTITY_SELECTION_TYPE);
            IdcEntityOrderBy orderBy = (IdcEntityOrderBy) params.get(MdekKeys.REQUESTINFO_ENTITY_ORDER_BY);
            Boolean orderAsc = (Boolean) params.get(MdekKeys.REQUESTINFO_ENTITY_ORDER_ASC);
            Integer startHit = (Integer) params.get(MdekKeys.REQUESTINFO_START_HIT);
            Integer numHits = (Integer) params.get(MdekKeys.REQUESTINFO_NUM_HITS);

            daoObjectNode.beginTransaction();
            daoObjectNode.disableAutoFlush();

            IngridDocument result = daoObjectNode.getWorkObjects(userUuid,
                    selectionType, orderBy, orderAsc,
                    startHit, numHits);

            @SuppressWarnings("unchecked")
            List<ObjectNode> oNs = (List<ObjectNode>) result.get(MdekKeys.OBJ_ENTITIES);
            Long totalNumPaging = (Long) result.get(MdekKeys.TOTAL_NUM_PAGING);
            Long totalNumAssigned = (Long) result.get(MdekKeys.TOTAL_NUM_QA_ASSIGNED);
            Long totalNumReassigned = (Long) result.get(MdekKeys.TOTAL_NUM_QA_REASSIGNED);

            // map found objects and related user addresses to docs
            ArrayList<IngridDocument> oNDocs = new ArrayList<IngridDocument>(oNs.size());
            for (int i = 0; i < oNs.size(); i++) {
                ObjectNode oN = oNs.get(i);
                T01Object o;
                // EXPIRED queries PUBLISHED version !
                // PORTAL_QUICKLIST_PUBLISHED queries PUBLISHED version !
                if (selectionType == IdcWorkEntitiesSelectionType.EXPIRED ||
                        selectionType == IdcWorkEntitiesSelectionType.PORTAL_QUICKLIST_PUBLISHED) {
                    o = oN.getT01ObjectPublished();
                } else {
                    o = oN.getT01ObjectWork();
                }

                IngridDocument objDoc =
                        beanToDocMapper.mapT01Object(o, new IngridDocument(), MappingQuantity.BASIC_ENTITY);

                // DEBUGGING for tests !
                if (log.isDebugEnabled()) {
                    // map some user uuids for debugging on client side !
                    beanToDocMapper.mapModUser(o.getModUuid(), objDoc, MappingQuantity.INITIAL_ENTITY);
                    beanToDocMapper.mapResponsibleUser(o.getResponsibleUuid(), objDoc, MappingQuantity.INITIAL_ENTITY);
                    beanToDocMapper.mapAssignerUser(o.getObjectMetadata().getAssignerUuid(), objDoc, MappingQuantity.INITIAL_ENTITY);
                }

                // map details according to selection !
                if (selectionType == IdcWorkEntitiesSelectionType.EXPIRED ||
                        selectionType == IdcWorkEntitiesSelectionType.MODIFIED ||
                        selectionType == IdcWorkEntitiesSelectionType.SPATIAL_REF_EXPIRED) {
                    // map mod user
                    AddressNode modAddressNode = o.getAddressNodeMod();
                    if (modAddressNode != null) {
                        beanToDocMapper.mapModUser(modAddressNode.getT02AddressWork(), objDoc);
                    }
                }
                if (selectionType == IdcWorkEntitiesSelectionType.MODIFIED ||
                        selectionType == IdcWorkEntitiesSelectionType.IN_QA_WORKFLOW ||
                        selectionType == IdcWorkEntitiesSelectionType.PORTAL_QUICKLIST ||
                        selectionType == IdcWorkEntitiesSelectionType.PORTAL_QUICKLIST_PUBLISHED) {
                    beanToDocMapper.mapUserOperation(oN, objDoc);
                }
                if (selectionType == IdcWorkEntitiesSelectionType.IN_QA_WORKFLOW) {
                    // map assigner user !
                    beanToDocMapper.mapObjectMetadata(o.getObjectMetadata(), objDoc, MappingQuantity.DETAIL_ENTITY);
                }
                if (selectionType == IdcWorkEntitiesSelectionType.PORTAL_QUICKLIST_ALL_USERS) {
                    // add permissions the user has on given object !
                    List<Permission> perms =
                            permissionHandler.getPermissionsForObject(oN.getObjUuid(), userUuid, true);
                    beanToDocMapperSecurity.mapPermissionList(perms, objDoc);
                }
                if (selectionType == IdcWorkEntitiesSelectionType.PORTAL_QUICKLIST_ALL_USERS_PUBLISHED) {
                    beanToDocMapper.mapObjectMetadata(o.getObjectMetadata(), objDoc, MappingQuantity.DETAIL_ENTITY);
                }

                oNDocs.add(objDoc);
            }

            // set up result
            result = new IngridDocument();
            result.put(MdekKeys.TOTAL_NUM_PAGING, totalNumPaging);
            result.put(MdekKeys.TOTAL_NUM_QA_ASSIGNED, totalNumAssigned);
            result.put(MdekKeys.TOTAL_NUM_QA_REASSIGNED, totalNumReassigned);
            result.put(MdekKeys.OBJ_ENTITIES, oNDocs);

            daoObjectNode.commitTransaction();
            return result;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            throw handledExc;
        }
    }

    public IngridDocument getQAObjects(IngridDocument params) {
        String userUuid = getCurrentUserUuid(params);
        try {
            WorkState whichWorkState = (WorkState) params.get(MdekKeys.REQUESTINFO_WHICH_WORK_STATE);
            IdcQAEntitiesSelectionType selectionType =
                    (IdcQAEntitiesSelectionType) params.get(MdekKeys.REQUESTINFO_ENTITY_SELECTION_TYPE);
            IdcEntityOrderBy orderBy = (IdcEntityOrderBy) params.get(MdekKeys.REQUESTINFO_ENTITY_ORDER_BY);
            Boolean orderAsc = (Boolean) params.get(MdekKeys.REQUESTINFO_ENTITY_ORDER_ASC);
            Integer startHit = (Integer) params.get(MdekKeys.REQUESTINFO_START_HIT);
            Integer numHits = (Integer) params.get(MdekKeys.REQUESTINFO_NUM_HITS);

            daoObjectNode.beginTransaction();
            daoObjectNode.disableAutoFlush();

            boolean isCatAdmin = permissionHandler.isCatalogAdmin(userUuid);

            IngridDocument result =
                    daoObjectNode.getQAObjects(userUuid, isCatAdmin, permissionHandler,
                            whichWorkState, selectionType,
                            orderBy, orderAsc,
                            startHit, numHits);

            @SuppressWarnings("unchecked")
            List<ObjectNode> oNs = (List<ObjectNode>) result.get(MdekKeys.OBJ_ENTITIES);
            Long totalNumPaging = (Long) result.get(MdekKeys.TOTAL_NUM_PAGING);

            // map found objects to docs
            ArrayList<IngridDocument> oNDocs = new ArrayList<IngridDocument>(oNs.size());
            for (ObjectNode oN : oNs) {
                T01Object o = oN.getT01ObjectWork();
                IngridDocument objDoc =
                        beanToDocMapper.mapT01Object(o, new IngridDocument(), MappingQuantity.BASIC_ENTITY);
                // map details according to selection (according to displayed data on QA page !)
                if (whichWorkState == WorkState.QS_UEBERWIESEN) {
                    // map assigner user !
                    beanToDocMapper.mapObjectMetadata(o.getObjectMetadata(), objDoc, MappingQuantity.DETAIL_ENTITY);
                } else {
                    // map mod user !
                    beanToDocMapper.mapObjectMetadata(o.getObjectMetadata(), objDoc, MappingQuantity.BASIC_ENTITY);
                    beanToDocMapper.mapModUser(o.getModUuid(), objDoc, MappingQuantity.DETAIL_ENTITY);
                }
                if (selectionType != IdcQAEntitiesSelectionType.EXPIRED &&
                        selectionType != IdcQAEntitiesSelectionType.SPATIAL_REF_EXPIRED) {
                    beanToDocMapper.mapUserOperation(oN, objDoc);
                }

                oNDocs.add(objDoc);
            }

            // set up result
            result = new IngridDocument();
            result.put(MdekKeys.TOTAL_NUM_PAGING, totalNumPaging);
            result.put(MdekKeys.OBJ_ENTITIES, oNDocs);

            daoObjectNode.commitTransaction();
            return result;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            throw handledExc;
        }
    }

    public IngridDocument getObjectStatistics(IngridDocument params) {
//		String userUuid = getCurrentUserUuid(params);
        try {
            String parentUuid = (String) params.get(MdekKeys.UUID);
            IdcStatisticsSelectionType selectionType =
                    (IdcStatisticsSelectionType) params.get(MdekKeys.REQUESTINFO_ENTITY_SELECTION_TYPE);
            Integer startHit = (Integer) params.get(MdekKeys.REQUESTINFO_START_HIT);
            Integer numHits = (Integer) params.get(MdekKeys.REQUESTINFO_NUM_HITS);

            daoObjectNode.beginTransaction();
            daoObjectNode.disableAutoFlush();

            IngridDocument result =
                    daoObjectNode.getObjectStatistics(parentUuid, selectionType, startHit, numHits);

            daoObjectNode.commitTransaction();
            return result;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            throw handledExc;
        }
    }

    public IngridDocument storeObject(IngridDocument oDocIn) {
        String userId = getCurrentUserUuid(oDocIn);
        boolean removeRunningJob = true;
        try {
            // first add basic running jobs info !
            addRunningJob(userId, createRunningJobDescription(JobType.STORE, 0, 1, false));

            Boolean refetchAfterStore = (Boolean) oDocIn.get(MdekKeys.REQUESTINFO_REFETCH_ENTITY);

            daoObjectNode.beginTransaction();

            // set specific data to transfer to working copy and store !
            workflowHandler.processDocOnStore(oDocIn);
            // Set default values for the metadata standard name and version
            setDefaultMetadataStandardProperties(oDocIn);
            String uuid = objectService.storeWorkingCopy(oDocIn, userId);

            // COMMIT BEFORE REFETCHING !!! otherwise we get old data !
            daoObjectNode.commitTransaction();

            // return uuid (may be new generated uuid if new object)
            IngridDocument result = new IngridDocument();
            result.put(MdekKeys.UUID, uuid);

            if (refetchAfterStore) {
                daoObjectNode.beginTransaction();
                result = getObjDetails(uuid, userId);
                daoObjectNode.commitTransaction();

                if (log.isDebugEnabled()) {
                    if (!MdekIdcEntityComparer.compareObjectMaps(oDocIn, result, null)) {
                        log.debug("Differences in Documents after store/refetch detected!");
                    }
                }
            }

            return result;

        } catch (RuntimeException e) {
            log.error(ExceptionUtils.getStackTrace(e));
            RuntimeException handledExc = handleException(e);
            removeRunningJob = errorHandler.shouldRemoveRunningJob(handledExc);
            throw handledExc;
        } finally {
            if (removeRunningJob) {
                removeRunningJob(userId);
            }
        }
    }

    public IngridDocument assignObjectToQA(IngridDocument oDocIn) {
        String userId = getCurrentUserUuid(oDocIn);
        boolean removeRunningJob = true;
        try {
            // first add basic running jobs info !
            addRunningJob(userId, createRunningJobDescription(JobType.STORE, 0, 1, false));

            Boolean refetchAfterStore = (Boolean) oDocIn.get(MdekKeys.REQUESTINFO_REFETCH_ENTITY);

            daoObjectNode.beginTransaction();

            String uuid = objectService.assignObjectToQA(oDocIn, userId);

            // COMMIT BEFORE REFETCHING !!! otherwise we get old data !
            daoObjectNode.commitTransaction();

            // return uuid (may be new generated uuid if new object)
            IngridDocument result = new IngridDocument();
            result.put(MdekKeys.UUID, uuid);

            if (refetchAfterStore) {
                daoObjectNode.beginTransaction();
                result = getObjDetails(uuid, userId);
                daoObjectNode.commitTransaction();

                if (log.isDebugEnabled()) {
                    if (!MdekIdcEntityComparer.compareObjectMaps(oDocIn, result, null)) {
                        log.debug("Differences in Documents after store/refetch detected!");
                    }
                }
            }

            return result;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            removeRunningJob = errorHandler.shouldRemoveRunningJob(handledExc);
            throw handledExc;
        } finally {
            if (removeRunningJob) {
                removeRunningJob(userId);
            }
        }
    }

    public IngridDocument reassignObjectToAuthor(IngridDocument oDocIn) {
        String userId = getCurrentUserUuid(oDocIn);
        boolean removeRunningJob = true;
        try {
            // first add basic running jobs info !
            addRunningJob(userId, createRunningJobDescription(JobType.STORE, 0, 1, false));

            Boolean refetchAfterStore = (Boolean) oDocIn.get(MdekKeys.REQUESTINFO_REFETCH_ENTITY);

            daoObjectNode.beginTransaction();

            // set common data to transfer to working copy !
            workflowHandler.processDocOnReassignToAuthor(oDocIn, userId);
            String uuid = objectService.storeWorkingCopy(oDocIn, userId);

            // COMMIT BEFORE REFETCHING !!! otherwise we get old data !?
            daoObjectNode.commitTransaction();

            // return uuid (may be new generated uuid if new object)
            IngridDocument result = new IngridDocument();
            result.put(MdekKeys.UUID, uuid);

            if (refetchAfterStore) {
                daoObjectNode.beginTransaction();
                result = getObjDetails(uuid, userId);
                daoObjectNode.commitTransaction();

                if (log.isDebugEnabled()) {
                    if (!MdekIdcEntityComparer.compareObjectMaps(oDocIn, result, null)) {
                        log.debug("Differences in Documents after store/refetch detected!");
                    }
                }
            }

            return result;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            removeRunningJob = errorHandler.shouldRemoveRunningJob(handledExc);
            throw handledExc;
        } finally {
            if (removeRunningJob) {
                removeRunningJob(userId);
            }
        }
    }

    public IngridDocument updateObjectPart(IngridDocument oPartDocIn) {
        String userId = getCurrentUserUuid(oPartDocIn);
        boolean removeRunningJob = true;
        try {
            // first add basic running jobs info !
            addRunningJob(userId, createRunningJobDescription(JobType.STORE, 0, 1, false));

            String uuid = (String) oPartDocIn.get(MdekKeys.UUID);
            IdcEntityVersion whichEntityVersion =
                    (IdcEntityVersion) oPartDocIn.get(MdekKeys.REQUESTINFO_WHICH_ENTITY_VERSION);

            daoObjectNode.beginTransaction();

            // check permissions !
            permissionHandler.checkWritePermissionForObject(uuid, userId, true);

            // load node
            ObjectNode oNode = objectService.loadByUuid(uuid, whichEntityVersion);
            if (oNode == null) {
                throw new MdekException(new MdekError(MdekErrorType.UUID_NOT_FOUND));
            }

            // fetch versions to update
            Long oWorkId = null;
            if (whichEntityVersion == IdcEntityVersion.WORKING_VERSION ||
                    whichEntityVersion == IdcEntityVersion.ALL_VERSIONS) {
                oWorkId = oNode.getObjId();
            }
            Long oPubId = null;
            if (whichEntityVersion == IdcEntityVersion.PUBLISHED_VERSION ||
                    whichEntityVersion == IdcEntityVersion.ALL_VERSIONS) {
                oPubId = oNode.getObjIdPublished();
            }

            // return null result if no update
            IngridDocument result = null;

            // update work version if requested
            if (oWorkId != null) {
                result = updateObjectPart(oNode.getT01ObjectWork(), oPartDocIn);
            }

            // update pub version if necessary
            if (oPubId != null && !oPubId.equals(oWorkId)) {
                result = updateObjectPart(oNode.getT01ObjectPublished(), oPartDocIn);
            }

            daoObjectNode.commitTransaction();

            return result;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            removeRunningJob = errorHandler.shouldRemoveRunningJob(handledExc);
            throw handledExc;
        } finally {
            if (removeRunningJob) {
                removeRunningJob(userId);
            }
        }
    }

    private IngridDocument updateObjectPart(T01Object o, IngridDocument oPartDocIn) {
        if (oPartDocIn.get(MdekKeys.EXPIRY_STATE) != null) {
            o.getObjectMetadata().setExpiryState((Integer) oPartDocIn.get(MdekKeys.EXPIRY_STATE));
        }
        if (oPartDocIn.get(MdekKeys.LASTEXPIRY_TIME) != null) {
            o.getObjectMetadata().setLastexpiryTime((String) oPartDocIn.get(MdekKeys.LASTEXPIRY_TIME));
        }
        if (oPartDocIn.get(MdekKeys.TITLE) != null) {
            o.setObjName((String) oPartDocIn.get(MdekKeys.TITLE));
        }
        daoT01Object.makePersistent(o);

        // not null indicates update executed
        return new IngridDocument();
    }

    public IngridDocument publishObject(IngridDocument oDocIn) {
        String userId = getCurrentUserUuid(oDocIn);
        boolean removeRunningJob = true;
        try {
            // first add basic running jobs info !
            addRunningJob(userId, createRunningJobDescription(JobType.PUBLISH, 0, 1, false));

            Boolean refetchAfterStore = (Boolean) oDocIn.get(MdekKeys.REQUESTINFO_REFETCH_ENTITY);
            Boolean forcePubCondition = (Boolean) oDocIn.get(MdekKeys.REQUESTINFO_FORCE_PUBLICATION_CONDITION);

            daoObjectNode.beginTransaction();

            // Set default values for the metadata standard name and version
            setDefaultMetadataStandardProperties(oDocIn);
            String uuid = objectService.publishObject(oDocIn, forcePubCondition, userId);

            // COMMIT BEFORE REFETCHING !!! otherwise we get old data !?
            daoObjectNode.commitTransaction();

            // Update search index with data of all published objects and also log if set
            updateSearchIndexAndAudit(jobHandler.getRunningJobChangedEntities(userId));

            IngridDocument result = new IngridDocument();
            result.put(MdekKeys.UUID, uuid);

            if (refetchAfterStore) {
                daoObjectNode.beginTransaction();
                result = getObjDetails(uuid, userId);
                daoObjectNode.commitTransaction();

                if (log.isDebugEnabled()) {
                    if (!MdekIdcEntityComparer.compareObjectMaps(oDocIn, result, null)) {
                        log.debug("Differences in Documents after publish/refetch detected!");
                    }
                }
            }

            return result;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            removeRunningJob = errorHandler.shouldRemoveRunningJob(handledExc);
            throw handledExc;
        } finally {
            if (removeRunningJob) {
                removeRunningJob(userId);
            }
        }
    }

    /**
     * DELETE ONLY WORKING COPY.
     * Notice: If no published version exists the object is deleted completely, meaning non existent afterwards
     * (including all subobjects !)
     */
    public IngridDocument deleteObjectWorkingCopy(IngridDocument params) {
        String userId = getCurrentUserUuid(params);
        boolean removeRunningJob = true;
        try {
            // first add basic running jobs info !
            addRunningJob(userId, createRunningJobDescription(JobType.DELETE, 0, 1, false));

            String uuid = (String) params.get(MdekKeys.UUID);
            Boolean forceDeleteReferences = (Boolean) params.get(MdekKeys.REQUESTINFO_FORCE_DELETE_REFERENCES);

            daoObjectNode.beginTransaction();

            IngridDocument result = objectService.deleteObjectWorkingCopy(uuid, forceDeleteReferences, userId);

            daoObjectNode.commitTransaction();

            return result;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            removeRunningJob = errorHandler.shouldRemoveRunningJob(handledExc);
            throw handledExc;
        } finally {
            if (removeRunningJob) {
                removeRunningJob(userId);
            }
        }
    }

    /**
     * FULL DELETE: different behavior when workflow enabled<br>
     * - QA: full delete of object (working copy and published version) INCLUDING all subobjects !
     * Object is non existent afterwards !<br>
     * - NON QA: object is just marked deleted and assigned to QA<br>
     * If workflow disabled every user acts like a QA (when having write access)
     */
    public IngridDocument deleteObject(IngridDocument params) {
        String userId = getCurrentUserUuid(params);
        boolean removeRunningJob = true;
        boolean transactionInProgress = (boolean) getOrDefault(params, MdekKeys.REQUESTINFO_TRANSACTION_IS_HANDLED, false);
        try {
            // first add basic running jobs info !
            addRunningJob(userId, createRunningJobDescription(JobType.DELETE, 0, 1, false));

            String uuid = (String) params.get(MdekKeys.UUID);
            Boolean forceDeleteReferences = (Boolean) params.get(MdekKeys.REQUESTINFO_FORCE_DELETE_REFERENCES);
            Boolean byOrigId = params.containsKey(MdekKeys.REQUESTINFO_USE_ORIG_ID) ? (Boolean) params.get(MdekKeys.REQUESTINFO_USE_ORIG_ID) : false;

            if (!transactionInProgress) {
                daoObjectNode.beginTransaction();
            }

            IngridDocument result = null;
            if (byOrigId == true) {
                result = objectService.deleteObjectByOrigId(uuid, forceDeleteReferences, userId);
            } else {
                result = objectService.deleteObjectFull(uuid, forceDeleteReferences, userId);
            }

            if (!transactionInProgress) {
                // Update search index
                updateSearchIndexAndAudit(jobHandler.getRunningJobChangedEntities(userId));

                daoObjectNode.commitTransaction();
            }

            return result;

        } catch (RuntimeException e) {
            log.error("Error deleting object", e);
            RuntimeException handledExc = handleException(e, transactionInProgress);
            removeRunningJob = errorHandler.shouldRemoveRunningJob(handledExc);
            throw handledExc;
        } catch (Exception e) {
            log.error("Exception when deleting object", e);
            throw new RuntimeException(e.getMessage());
        } finally {
            if (removeRunningJob) {
                removeRunningJob(userId);
            }
        }
    }

    /**
     * Move Object with its subtree to new parent.
     */
    public IngridDocument moveObject(IngridDocument params) {
        String userUuid = getCurrentUserUuid(params);
        boolean removeRunningJob = true;
        try {
            // first add basic running jobs info !
            addRunningJob(userUuid, createRunningJobDescription(JobType.MOVE, 0, 1, false));

            String fromUuid = (String) params.get(MdekKeys.FROM_UUID);
            String toUuid = (String) params.get(MdekKeys.TO_UUID);
            Boolean forcePubCondition = (Boolean) params.get(MdekKeys.REQUESTINFO_FORCE_PUBLICATION_CONDITION);

            daoObjectNode.beginTransaction();

            IngridDocument resultDoc = objectService.moveObject(
                    fromUuid, toUuid, forcePubCondition, userUuid);

            // add permissions to result
            List<Permission> perms = permissionHandler.getPermissionsForObject(fromUuid, userUuid, true);
            beanToDocMapperSecurity.mapPermissionList(perms, resultDoc);

            daoObjectNode.commitTransaction();

            return resultDoc;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            removeRunningJob = errorHandler.shouldRemoveRunningJob(handledExc);
            throw handledExc;
        } finally {
            if (removeRunningJob) {
                removeRunningJob(userUuid);
            }
        }
    }

    /**
     * Checks whether subtree of object has working copies.
     */
    public IngridDocument checkObjectSubTree(IngridDocument params) {
        String userId = getCurrentUserUuid(params);
        boolean removeRunningJob = true;
        try {
            // first add basic running jobs info !
            addRunningJob(userId, createRunningJobDescription(JobType.CHECK, 0, 1, false));

            String rootUuid = (String) params.get(MdekKeys.UUID);

            daoObjectNode.beginTransaction();
            daoObjectNode.disableAutoFlush();

            IngridDocument checkResult = checkObjectTreeWorkingCopies(rootUuid);

            daoObjectNode.commitTransaction();
            return checkResult;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            removeRunningJob = errorHandler.shouldRemoveRunningJob(handledExc);
            throw handledExc;
        } finally {
            if (removeRunningJob) {
                removeRunningJob(userId);
            }
        }
    }

    /**
     * Checks whether object branch has working copies (passed root is also checked !).
     */
    private IngridDocument checkObjectTreeWorkingCopies(String rootUuid) {
        // load "root"
        ObjectNode rootNode = objectService.loadByUuid(rootUuid, null);
        if (rootNode == null) {
            throw new MdekException(new MdekError(MdekErrorType.UUID_NOT_FOUND));
        }

        // process all subnodes including root

        List<ObjectNode> subNodes = daoObjectNode.getAllSubObjects(rootUuid, null, false);
        subNodes.add(0, rootNode);

        boolean hasWorkingCopy = false;
        String uuidOfWorkingCopy = null;
        int numberOfCheckedObj = 0;

        for (ObjectNode subNode : subNodes) {
            numberOfCheckedObj++;

            if (!subNode.getObjId().equals(subNode.getObjIdPublished())) {
                hasWorkingCopy = true;
                uuidOfWorkingCopy = subNode.getObjUuid();
                break;
            }
        }
        if (log.isDebugEnabled()) {
            log.debug("Number of checked objects: " + numberOfCheckedObj);
        }

        IngridDocument result = new IngridDocument();
        result.put(MdekKeys.RESULTINFO_HAS_WORKING_COPY, hasWorkingCopy);
        result.put(MdekKeys.RESULTINFO_UUID_OF_FOUND_ENTITY, uuidOfWorkingCopy);
        result.put(MdekKeys.RESULTINFO_NUMBER_OF_PROCESSED_ENTITIES, numberOfCheckedObj);

        return result;
    }

    /**
     * Copy Object to new parent (with or without its subtree). Returns basic data of copied top object.
     */
    public IngridDocument copyObject(IngridDocument params) {
        String userUuid = getCurrentUserUuid(params);
        boolean removeRunningJob = true;
        try {
            // first add basic running jobs info !
            addRunningJob(userUuid, createRunningJobDescription(JobType.COPY, 0, 1, false));

            daoObjectNode.beginTransaction();

            String fromUuid = (String) params.get(MdekKeys.FROM_UUID);
            String toUuid = (String) params.get(MdekKeys.TO_UUID);
            Boolean copySubtree = (Boolean) params.get(MdekKeys.REQUESTINFO_COPY_SUBTREE);

            // check permissions !
            permissionHandler.checkPermissionsForCopyObject(fromUuid, toUuid, userUuid);

            // perform checks
            ObjectNode fromNode = objectService.loadByUuid(fromUuid, IdcEntityVersion.WORKING_VERSION);
            if (fromNode == null) {
                throw new MdekException(new MdekError(MdekErrorType.FROM_UUID_NOT_FOUND));
            }

            ObjectNode toNode = null;
            // NOTICE: copy to top when toUuid is null
            if (toUuid != null) {
                toNode = objectService.loadByUuid(toUuid, null);
                if (toNode == null) {
                    throw new MdekException(new MdekError(MdekErrorType.TO_UUID_NOT_FOUND));
                }
            }

            // copy fromNode
            IngridDocument copyResult = createObjectNodeCopy(fromNode, toNode, copySubtree, userUuid);
            ObjectNode fromNodeCopy = (ObjectNode) copyResult.get(MdekKeys.OBJ_ENTITIES);
            Integer numCopiedObjects = (Integer) copyResult.get(MdekKeys.RESULTINFO_NUMBER_OF_PROCESSED_ENTITIES);
            if (log.isDebugEnabled()) {
                log.debug("Number of copied objects: " + numCopiedObjects);
            }

            // success
            IngridDocument resultDoc = new IngridDocument();
            beanToDocMapper.mapT01Object(fromNodeCopy.getT01ObjectWork(), resultDoc, MappingQuantity.TABLE_ENTITY);
            // also child info
            beanToDocMapper.mapObjectNode(fromNodeCopy, resultDoc, MappingQuantity.COPY_ENTITY);
            // and additional info
            resultDoc.put(MdekKeys.RESULTINFO_NUMBER_OF_PROCESSED_ENTITIES, numCopiedObjects);

            // grant write tree permission (e.g. if new root node)
            if (toUuid == null) {
                // NEW ROOT NODE: grant write-tree permission, no permission yet ! 
                permissionHandler.grantTreePermissionForObject(fromNodeCopy.getObjUuid(), userUuid,
                        GroupType.ONLY_GROUPS_WITH_CREATE_ROOT_PERMISSION, null);
            } else if (permissionHandler.hasSubNodePermissionForObject(toUuid, userUuid, false)) {
                // NEW NODE UNDER PARENT WITH SUBNODE PERMISSION: grant write-tree permission only for special group ! 
                permissionHandler.grantTreePermissionForObject(fromNodeCopy.getObjUuid(), userUuid,
                        GroupType.ONLY_GROUPS_WITH_SUBNODE_PERMISSION_ON_OBJECT, toUuid);
            }

            // add permissions to result
            List<Permission> perms = permissionHandler.getPermissionsForObject(fromNodeCopy.getObjUuid(), userUuid, true);
            beanToDocMapperSecurity.mapPermissionList(perms, resultDoc);

            daoObjectNode.commitTransaction();
            return resultDoc;

        } catch (RuntimeException e) {
            RuntimeException handledExc = handleException(e);
            removeRunningJob = errorHandler.shouldRemoveRunningJob(handledExc);
            throw handledExc;
        } finally {
            if (removeRunningJob) {
                removeRunningJob(userUuid);
            }
        }
    }

    /**
     * Generate the IDF of the requested document.
     *
     * @param doc is the document to get ISO XML from
     * @returns the document as XML
     */
    public IngridDocument getIsoXml(IngridDocument doc) {
        String id = doc.getString(MdekKeys.UUID);
        IdcEntityVersion version = (IdcEntityVersion) doc.get(MdekKeys.REQUESTINFO_WHICH_ENTITY_VERSION);
        IngridDocument resultDoc = new IngridDocument();
        final IngridHit hit = new IngridHit();
        hit.setDocumentId(id);
        Record record = null;
        try {
            daoObjectNode.beginTransaction();
            ObjectNode objNode = objectService.loadByUuid(id, null);
            Long objId = null;
            if (version == IdcEntityVersion.WORKING_VERSION) {
                objId = objNode.getObjId();
            } else {
                objId = objNode.getObjIdPublished();
            }
            ElasticDocument elasticDocument = new ElasticDocument();
            elasticDocument.put("t01_object.id", objId);
            record = dscRecordProducer.getRecord(elasticDocument);

            //record = recordLoader.getRecord( hit );
            Document nodeDoc = MdekRecordUtils.convertRecordToDocument(record);
            String isoDocAsString = null;
            if (nodeDoc != null) {
                Node isoDoc = xsltUtils.transform(nodeDoc, MdekRecordUtils.XSL_IDF_TO_ISO_FULL);
                isoDocAsString = XMLUtils.toString((Document) isoDoc);
            }
            resultDoc.put("record", isoDocAsString);
        } catch (Exception e) {
            log.error("Could not get record with ID: " + id, e);
        } finally {
            daoObjectNode.commitTransaction();
        }
        return resultDoc;
    }

    /**
     * Creates a copy of the given ObjectNode and adds it under the given parent.
     * Copies ONLY working version and IGNORES published version !
     * Copied nodes are already Persisted !!!<br>
     * NOTICE: Also copies whole subtree dependent from passed flag.<br>
     * NOTICE: also supports copy of a tree to one of its subnodes !
     *
     * @param sourceNode    copy this node
     * @param newParentNode under this node
     * @param copySubtree   including subtree or not
     * @param userUuid      current user id needed to update running jobs
     * @return doc containing additional info (copy of source node, number copied objects ...)
     */
    @SuppressWarnings("unchecked")
    private IngridDocument createObjectNodeCopy(ObjectNode sourceNode, ObjectNode newParentNode,
                                                boolean copySubtree, String userUuid) {
        // refine running jobs info
        int totalNumToCopy = 1;
        if (copySubtree) {
            // total num to copy: root + sub objects
            totalNumToCopy = 1 +
                    daoObjectNode.countAllSubObjects(sourceNode.getObjUuid(), IdcEntityVersion.ALL_VERSIONS);
            updateRunningJob(userUuid, createRunningJobDescription(JobType.COPY, 0, totalNumToCopy, false));
        }

        // check whether we copy to subnode
        // then we have to check already copied nodes to avoid endless copy !
        boolean isCopyToOwnSubnode = false;
        ArrayList<String> uuidsCopiedNodes = null;
        if (newParentNode != null) {
            if (daoObjectNode.isSubNode(newParentNode.getObjUuid(), sourceNode.getObjUuid())) {
                isCopyToOwnSubnode = true;
                uuidsCopiedNodes = new ArrayList<String>();
            }
        }

        // copy iteratively via stack to avoid recursive stack overflow
        Stack<IngridDocument> stack = new Stack<IngridDocument>();
        IngridDocument nodeDoc = new IngridDocument();
        nodeDoc.put("NODE", sourceNode);
        nodeDoc.put("PARENT_NODE", newParentNode);
        stack.push(nodeDoc);

        int numberOfCopiedObj = 0;
        ObjectNode rootNodeCopy = null;
        while (!stack.isEmpty()) {
            nodeDoc = stack.pop();
            sourceNode = (ObjectNode) nodeDoc.get("NODE");
            newParentNode = (ObjectNode) nodeDoc.get("PARENT_NODE");

            if (log.isDebugEnabled()) {
                log.debug("Copying entity " + sourceNode.getObjUuid());
            }

            // copy source work version !
            String newUuid = EntityHelper.getInstance().generateUuid();
            T01Object targetObjWork = createT01ObjectCopy(sourceNode.getT01ObjectWork(), newUuid, userUuid);

            // create new Node and set data !
            // we also set Beans in object node, so we can access them afterwards.
            Long targetObjWorkId = targetObjWork.getId();
            String newParentUuid = null;
            if (newParentNode != null) {
                newParentUuid = newParentNode.getObjUuid();
            }

            ObjectNode targetNode = new ObjectNode();
            targetNode.setObjUuid(newUuid);
            targetNode.setObjId(targetObjWorkId);
            targetNode.setT01ObjectWork(targetObjWork);
            targetNode.setFkObjUuid(newParentUuid);

            // also care for tree path !
            pathHandler.setTreePath(targetNode, newParentNode);
            daoObjectNode.makePersistent(targetNode);

            // add child bean to parent bean, so we can determine child info when mapping (without reloading)
            if (newParentNode != null) {
                newParentNode.getObjectNodeChildren().add(targetNode);
            }

            if (rootNodeCopy == null) {
                rootNodeCopy = targetNode;
            }
            if (isCopyToOwnSubnode) {
                uuidsCopiedNodes.add(newUuid);
            }

            numberOfCopiedObj++;

            // update our job information ! may be polled from client !
            // NOTICE: also checks whether job was canceled !
            updateRunningJob(userUuid, createRunningJobDescription(
                    JobType.COPY, numberOfCopiedObj, totalNumToCopy, false));

            // copy subtree ? only if not already a copied node !
            if (copySubtree) {
                List<ObjectNode> sourceSubNodes = daoObjectNode.getSubObjects(sourceNode.getObjUuid(),
                        IdcEntityVersion.WORKING_VERSION, false);
                for (ObjectNode sourceSubNode : sourceSubNodes) {
                    if (isCopyToOwnSubnode) {
                        if (uuidsCopiedNodes.contains(sourceSubNode.getObjUuid())) {
                            // skip this node ! is one of our copied ones !
                            continue;
                        }
                    }

                    // add to stack, will be copied
                    nodeDoc = new IngridDocument();
                    nodeDoc.put("NODE", sourceSubNode);
                    nodeDoc.put("PARENT_NODE", targetNode);
                    stack.push(nodeDoc);
                }
            }
        }

        IngridDocument result = new IngridDocument();
        // copy of rootNode returned via OBJ_ENTITIES key !
        result.put(MdekKeys.OBJ_ENTITIES, rootNodeCopy);
        result.put(MdekKeys.RESULTINFO_NUMBER_OF_PROCESSED_ENTITIES, numberOfCopiedObj);

        return result;
    }

    /**
     * Creates a copy of the given T01Object with the given NEW uuid. Already Persisted !
     */
    private T01Object createT01ObjectCopy(T01Object sourceObj, String newUuid, String userUuid) {
        // create new object with new uuid and save it (to generate id !)
        T01Object targetObj = new T01Object();
        targetObj.setObjUuid(newUuid);
        daoT01Object.makePersistent(targetObj);

        // then copy content via mappers

        // map source bean to doc, ONLY data not specific to entity !
        IngridDocument targetObjDoc =
                beanToDocMapper.mapT01Object(sourceObj, new IngridDocument(), MappingQuantity.COPY_DATA);

        // update changed data in doc from source for target !
        targetObjDoc.put(MdekKeys.UUID, newUuid);
        String currentTime = MdekUtils.dateToTimestamp(new Date());
        targetObjDoc.put(MdekKeys.DATE_OF_CREATION, currentTime);
        targetObjDoc.put(MdekKeys.DATE_OF_LAST_MODIFICATION, currentTime);
        // REMOVE Original ID to avoid duplicate ! see INGRID-2299 
        targetObjDoc.remove(MdekKeys.ORIGINAL_CONTROL_IDENTIFIER);
        beanToDocMapper.mapModUser(userUuid, targetObjDoc, MappingQuantity.INITIAL_ENTITY);
        beanToDocMapper.mapResponsibleUser(userUuid, targetObjDoc, MappingQuantity.INITIAL_ENTITY);
        workflowHandler.processDocOnCopy(targetObjDoc);

        // remove "Identification of the data source" for copied objects (#1581)
        IngridDocument technicalDomainMap = (IngridDocument) targetObjDoc.get(MdekKeys.TECHNICAL_DOMAIN_MAP);
        if (technicalDomainMap != null) {
            technicalDomainMap.remove(MdekKeys.DATASOURCE_UUID);
        }

        // and transfer data from doc to new bean
        docToBeanMapper.mapT01Object(targetObjDoc, targetObj, MappingQuantity.COPY_ENTITY);

        daoT01Object.makePersistent(targetObj);

        return targetObj;
    }

    /**
     * Index a specific document
     */
    public IngridDocument updateObjectIndex(IngridDocument params) {
        String uuid = (String) params.get(MdekKeys.UUID);

        daoObjectNode.beginTransaction();
        ObjectNode objectNode = objectService.loadByUuid(uuid, IdcEntityVersion.PUBLISHED_VERSION);
        daoObjectNode.commitTransaction();

        if (objectNode == null) {
            throw new MdekException(new MdekError(MdekErrorType.UUID_NOT_FOUND));
        }

        T01Object publishedObject = objectNode.getT01ObjectPublished();
        String objId = publishedObject.getId().toString();
        DscDocumentProducer docProducer = docProducerObject;
        ElasticDocument doc = docProducer.getById(objId);
        updateSearchIndex(objId, doc, docProducer, true);

        IngridDocument result = new IngridDocument();
        result.put(MdekKeys.ID, objId);
        return result;
    }
}
